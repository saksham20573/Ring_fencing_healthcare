{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "o2tpqETLQsMn",
        "UOPGrwa2Q866",
        "_62z7vAvs6PP",
        "DBLUiqD-Heg7",
        "foj6Vwl0aNWn",
        "t0Wtyd4pd_2g"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook provides the code for the paper - \"Enhancing data security through ring fencing\""
      ],
      "metadata": {
        "id": "EToOvzpgKlxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the environment"
      ],
      "metadata": {
        "id": "UBCuUXrzT9E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install cryptography\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "tU8OUM5aUAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b53da8-acfd-48ca-c976-95d4ff4f0f24"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=512a33567da20489890b212f7214189f4428e5185323240443824ef2dad3f1af\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.13.3 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.27.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.9/dist-packages (40.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "qRhAk2YDONNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "import spacy\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn import preprocessing\n",
        "import time\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "from cryptography.hazmat.primitives import hashes\n",
        "from cryptography.hazmat.primitives.asymmetric import rsa, padding\n",
        "\n",
        "import sqlite3\n",
        "import predictionmodel"
      ],
      "metadata": {
        "id": "Ru2SYgzTOM9q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATE"
      ],
      "metadata": {
        "id": "HFPJ76i0RAOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "FbWCL0UULYoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_query = \"What is the bmi of the patient with id 1665 for Treatment\""
      ],
      "metadata": {
        "id": "i-FnRIXiR9JL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYJ2zONn16CC",
        "outputId": "1ed6c872-39f3-4cb4-9dac-c6be80cfb5d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_stroke = pd.read_csv(\"/content/drive/MyDrive/Ring_fencing_files/healthcare-dataset-stroke-data.csv\")\n",
        "df_stroke['bmi'].fillna(value = df_stroke['bmi'].mean(), inplace = True)"
      ],
      "metadata": {
        "id": "ticvaB5bjw0h"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the sql database to run queries"
      ],
      "metadata": {
        "id": "FVo70PLeA14X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "connection = sqlite3.connect(\"stroke.db\")\n",
        "crsr = connection.cursor()\n",
        "crsr.execute(\"\"\"CREATE TABLE stroke_table(\n",
        "   id                INTEGER NOT NULL PRIMARY KEY \n",
        "  ,gender            VARCHAR(6) NOT NULL\n",
        "  ,age               NUMERIC(4,2) NOT NULL\n",
        "  ,hypertension      BIT  NOT NULL\n",
        "  ,heart_disease     BIT  NOT NULL\n",
        "  ,ever_married      VARCHAR(3) NOT NULL\n",
        "  ,work_type         VARCHAR(13) NOT NULL\n",
        "  ,Residence_type    VARCHAR(5) NOT NULL\n",
        "  ,avg_glucose_level NUMERIC(6,2) NOT NULL\n",
        "  ,bmi               NUMERIC(4,1) NOT NULL\n",
        "  ,smoking_status    VARCHAR(15) NOT NULL\n",
        "  ,stroke            BIT  NOT NULL\n",
        ");\"\"\")\n",
        "\n",
        "file = open(\"/content/drive/MyDrive/Ring_fencing_files/stroke.txt\", \"r\")\n",
        "queries = file.readlines()\n",
        "for i in queries:\n",
        "  crsr.execute(i[:-1])"
      ],
      "metadata": {
        "id": "uskwmps4A04C"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hc5NIg_qKVoA"
      },
      "outputs": [],
      "source": [
        "# Defining the databases. Here we have stored the data in the form of python classes and dictionaries \n",
        "\n",
        "# This information will be provided by the hospital's data. This mock data has only been created for the purpose of this demonstration\n",
        "\n",
        "class User:\n",
        "  def __init__(self, id, name, institution, role, location):\n",
        "    self.id = id\n",
        "    self.name = name\n",
        "    self.institution = institution\n",
        "    self.role = role\n",
        "    self.location = location\n",
        "\n",
        "purpose_list_insurance = [\"Policy design\", \"Risk assessment\", \"Customer service\", \"Fraud detection\", \"Claims processing\"]\n",
        "purpose_list_hospital = [\"Treatment\", \"Reference\", \"Emergency\", \"Patient inquiry\"]\n",
        "purpose_list_research = [\"New model\", \"Paper publish\"]\n",
        "institution_list = [\"INSURANCE\", \"HOSPITAL\", \"RESEARCH\"]\n",
        "\n",
        "# list of roles in each institution\n",
        "roles_institutions = {\"INSURANCE\": [\"Actuarial_Analyst\", \"Insurance_Salesman\", \"Admin\", \"Investigator\"],\n",
        "                      \"HOSPITAL\": [\"Primary_Doctor\", \"Secondary_Doctor\", \"Admin\", \"Nurse\"],\n",
        "                      \"RESEARCH\": [\"Research_Intern\", \"Post_Doc_Fellow\", \"Professor\"]}\n",
        "\n",
        "# Query types allowed for institution\n",
        "institution_query_types = {\"INSURANCE\": [\"ANALYTICS\", \"INDIVIDUAL\"], \"HOSPITAL\": [\"ANALYTICS\", \"INDIVIDUAL\", \"PREDICTION\"], \"RESEARCH\": [\"ANALYTICS\", \"INDIVIDUAL\"]}\n",
        "\n",
        "# Query types allowed for roles\n",
        "purpose_role_insurance = {\"Actuarial_Analyst\": [\"ANALYTICS\"], \"Insurance_Salesman\": [], \"Admin\": [\"ANALYTICS\"], \"Investigator\": [\"ANALYTICS\", \"INDIVIDUAL\"]}\n",
        "purpose_role_hospital = {\"Primary_Doctor\": [\"ANALYTICS\", \"INDIVIDUAL\"], \"Secondary_Doctor\": [\"ANALYTICS\", \"INDIVIDUAL\"], \"Admin\": [\"ANALYTICS\", \"INDIVIDUAL\"], \"Nurse\": []}\n",
        "purpose_role_research = {\"Research_Intern\": [\"INDIVIDUAL\", \"ANALYTICS\"], \"Post_Doc_Fellow\": [\"ANALYTICS\"], \"Professor\": [\"ANALYTICS\"]}\n",
        "\n",
        "# Purpose table: \"Purpose\": [{\"roles\": \"types of competitor\"}, \"time_range\", \"locations\"]\n",
        "# purpose table for insurance company\n",
        "purpose_table_insurance = {\"Risk Assessment\": [[], [0, 0], []],\n",
        "                          \"Policy design\": [{\"Actuarial_Analyst\": [\"ANALYTICS\"]}, [9, 18], [\"Office\"]],\n",
        "                          \"Customer service\": [{\"Admin\": [\"ANALYTICS\"]}, [9, 18], [\"Office\"]],\n",
        "                          \"Fraud detection\": [{\"Investigator\": [\"ANALYTICS\", \"INDIVIDUAL\"]}, [9, 18], [\"Office\"]],\n",
        "                          \"Claims processing\": [{\"Investigator\": [\"ANALYTICS\", \"INDIVIDUAL\"]}, [9, 18], [\"Office\"]]}\n",
        "\n",
        "# purpose table for hospital\n",
        "purpose_table_hospital = {\"Treatment\": [{\"Primary_Doctor\": [\"INDIVIDUAL\", \"PREDICTION\"]}, [0, 24], [\"Hospital\", \"Home\"]],\n",
        "                          \"Reference\": [{\"Secondary_Doctor\": [\"INDIVIDUAL\"]}, [0, 24], [\"Hospital\", \"Home\"]],\n",
        "                          \"Emergency\": [{\"Primary_Doctor\": [\"INDIVIDUAL\", \"PREDICTION\"], \"Secondary_Doctor\": [\"INDIVIDUAL\", \"PREDICTION\"]}, [0, 24], [\"Hospital\"]],\n",
        "                          \"Patient inquiry\": [{\"Admin\": [\"INDIVIDUAL\", \"ANALYTICS\"]}, [9, 18], [\"Hospital\"]]}\n",
        "\n",
        "# purposes table for research\n",
        "purpose_table_research = {\"Treatment\": [{\"Research_Intern\": [\"INDIVIDUAL\", \"ANALYTICS\"]}, [9, 18], [\"Reasearch Lab\"]],\n",
        "                          \"Paper publish\": [{\"Professor\": [\"ANALYTICS\"], \"Post_Doc_Fellow\": [\"ANALYTICS\"]}, [9, 18], [\"Research Lab\"]]}\n",
        "\n",
        "curr_user = User(1, \"Annie\", \"HOSPITAL\", \"Primary_Doctor\", \"Hospital\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df_classify = pd.read_csv(\"/content/drive/MyDrive/Ring_fencing_files/Final_query_dataset - Sheet1.csv\")"
      ],
      "metadata": {
        "id": "rBBQd8jIN2_0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_classify_analysis = pd.read_csv(\"/content/drive/MyDrive/Ring_fencing_files/Analysis_Classification_dataset - Sheet1.csv\")"
      ],
      "metadata": {
        "id": "MA0jZHMEldc-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting the purpose through similarity matching"
      ],
      "metadata": {
        "id": "o2tpqETLQsMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the purpose list\n",
        "purpose_list = None\n",
        "if curr_user.institution == \"INSURANCE\":\n",
        "  purpose_list = purpose_list_insurance\n",
        "\n",
        "if curr_user.institution == \"HOSPITAL\":\n",
        "  purpose_list = purpose_list_hospital\n",
        "\n",
        "if curr_user.institution == \"RESEARCH\":\n",
        "  purpose_list = purpose_list_research"
      ],
      "metadata": {
        "id": "BopdnZEV2TYZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "query = unicodedata.normalize('NFKD', input_query)\n",
        "print(\"=================================\")\n",
        "print(\"Before breaking\")\n",
        "print(\"=================================\")\n",
        "print(query)\n",
        "\n",
        "lower_query = query.lower()\n",
        "q_purpose = \"\"\n",
        "\n",
        "break_ind = -1\n",
        "tags = []\n",
        "tokens = []\n",
        "purpose_found = False\n",
        "\n",
        "# POS tagging\n",
        "doc = nlp(query)\n",
        "for token in doc:\n",
        "    tokens.append(str(token))\n",
        "    tags.append((str(token), token.pos_))\n",
        "\n",
        "f = -1\n",
        "purpose_ = \"\"\n",
        "start_time = time.time()\n",
        "# Extracting purpose and breaking\n",
        "for purpose in purpose_list:\n",
        "  p = purpose.lower()\n",
        "  doc_purp = nlp(purpose)\n",
        "  token_purp = []\n",
        "  for i in doc_purp:\n",
        "    token_purp.append(str(i))\n",
        "  ngram = len(token_purp)\n",
        "  for i in range(len(tokens) - 1, ngram - 1, -1):\n",
        "    str1 = \" \".join(tokens[i - ngram + 1: i + 1])\n",
        "    str2 = \" \".join(token_purp)\n",
        "    samples = [str1, str2]\n",
        "    sentence_embeddings = model.encode(samples)\n",
        "    sim = cosine_similarity([sentence_embeddings[0]], sentence_embeddings[1:])\n",
        "    if float(sim[0][0]) > 0.9:\n",
        "      f = i - ngram + 1\n",
        "  if(f == -1):\n",
        "    continue\n",
        "  purpose_ = purpose \n",
        "  break\n",
        "end_time = time.time()\n",
        "print(\"Time taken to extract the purpose through similarity matching: \" + str(end_time - start_time))\n",
        "\n",
        "\n",
        "purpose = purpose_\n",
        "start_time = time.time()\n",
        "# If purpose not found\n",
        "if f == -1:\n",
        "  purpose = \"General Purpose\"\n",
        "  seg1 = query  \n",
        "\n",
        "else:\n",
        "  # Purpose found\n",
        "  purpose_found = True\n",
        "  print(\"Purpose = \" + purpose)\n",
        "  print()\n",
        "\n",
        "  # Breaking the query\n",
        "  doc = nlp(query)\n",
        "  seg1 = \"\"\n",
        "\n",
        "  # Rule based query division\n",
        "  for i in range(len(doc) - 1, -1, -1):\n",
        "    # print(i)\n",
        "    if tags[i][1] == \"ADP\" or tags[i][0].lower() == \"to\":\n",
        "      seg1 = tokens[:i]\n",
        "\n",
        "      # Printing the Second Segement\n",
        "      seg2 = tokens[i:]\n",
        "      break\n",
        "\n",
        "end_time = time.time()\n",
        "query_without_purpose = \" \".join(seg1)\n",
        "extracted_purpose = purpose\n",
        "print(\"Time taken to extract purpose: \" + str(end_time - start_time))"
      ],
      "metadata": {
        "id": "bj_fN8MMQw4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11675057-75fd-408d-beeb-0d65f401cda1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================\n",
            "Before breaking\n",
            "=================================\n",
            "What is the bmi of the patient with id 1665 for Treatment\n",
            "Time taken to extract the purpose through similarity matching: 0.9815773963928223\n",
            "Purpose = Treatment\n",
            "\n",
            "Time taken to extract purpose: 0.010278463363647461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifying the query"
      ],
      "metadata": {
        "id": "UOPGrwa2Q866"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_emb(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the bert embedding vectors for the each sentence column.\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "  bert_model = SentenceTransformer('distilbert-base-uncased')\n",
        "  \n",
        "  feature = bert_model.encode(data_frame)\n",
        "  \n",
        "  return feature"
      ],
      "metadata": {
        "id": "T_UK6dGZ4lR-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the classification model\n",
        "\n",
        "for i in df_classify.index:\n",
        "  if df_classify[\"LABEL\"][i] == \"ANALYTICS\":\n",
        "    df_classify[\"LABEL\"][i] = 0\n",
        "  if df_classify[\"LABEL\"][i] == \"INDIVIDUAL\":\n",
        "    df_classify[\"LABEL\"][i] = 1\n",
        "  if df_classify[\"LABEL\"][i] == \"PREDICTION\":\n",
        "    df_classify[\"LABEL\"][i] = 2\n",
        "\n",
        "y_train_classify = df_classify[\"LABEL\"]\n",
        "X_train_classify = df_classify[\"QUERY\"]\n",
        "\n",
        "classification_embeddings = get_bert_emb(np.array(X_train_classify))\n",
        "query_classifier = LogisticRegression(max_iter = 500)\n",
        "query_classifier.fit(np.array(classification_embeddings), np.array(y_train_classify, dtype = \"int\"))"
      ],
      "metadata": {
        "id": "d0f7AFd5Nyc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "18c81936-59c4-4692-b60c-1690d2ba0ef8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=500)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "query_class = query_classifier.predict(get_bert_emb(query_without_purpose).reshape(1, -1))\n",
        "end_time = time.time()\n",
        "print(\"Time taken to classify query: \" + str(end_time - start_time))\n",
        "query_class_text = \"\"\n",
        "if query_class == 0:\n",
        "  query_class_text = \"ANALYTICS\"\n",
        "elif query_class == 1:\n",
        "  query_class_text = \"INDIVIDUAL\"\n",
        "else:\n",
        "  query_class_text = \"PREDICTION\""
      ],
      "metadata": {
        "id": "y-r5fliDPM9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a59bd8-3482-4915-8bae-4d57fee6a99a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to classify query: 2.1404991149902344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting Analysis class\n",
        "query_analysis_class = None\n",
        "if query_class == 0:\n",
        "  for i in df_classify_analysis.index:\n",
        "    if df_classify_analysis[\"LABEL\"][i] == \"COUNT\":\n",
        "      df_classify_analysis[\"LABEL\"][i] = 0\n",
        "    # The probability class also includes queries related to percentages\n",
        "    if df_classify_analysis[\"LABEL\"][i] == \"PROBABILITY\":\n",
        "      df_classify_analysis[\"LABEL\"][i] = 1\n",
        "    if df_classify_analysis[\"LABEL\"][i] == \"CORRELATION\":\n",
        "      df_classify_analysis[\"LABEL\"][i] = 2\n",
        "  y_train_classify = df_classify_analysis[\"LABEL\"]\n",
        "  X_train_classify = df_classify_analysis[\"QUERY\"]\n",
        "\n",
        "  analysis_class_emb = get_bert_emb(np.array(X_train_classify))\n",
        "  ana_classifier = LogisticRegression(max_iter = 500)\n",
        "  ana_classifier.fit(np.array(analysis_class_emb), np.array(y_train_classify, dtype = \"int\"))\n",
        "  start_time = time.time()\n",
        "  query_analysis_class = ana_classifier.predict(get_bert_emb(query_without_purpose).reshape(1, -1))\n",
        "  end_time = time.time()\n",
        "  print(\"Time taken to run analytics classification\" + str(end_time - start_time))"
      ],
      "metadata": {
        "id": "fx7HelStlkep"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_analysis_class_text = \"\"\n",
        "if query_analysis_class == 0:\n",
        "  query_analysis_class_text = \"COUNT\"\n",
        "elif query_analysis_class == 1:\n",
        "  query_analysis_class_text = \"PROBABILITY\"\n",
        "elif query_analysis_class == 2:\n",
        "  query_analysis_class_text = \"CORRELATION\"\n",
        "print(\"Query class: \" + query_class_text)\n",
        "print(\"Analysis class: \" + query_analysis_class_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3DUX0Kl6GfB",
        "outputId": "6a2f1aed-ade2-4d14-f10a-69bcc6e67361"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query class: INDIVIDUAL\n",
            "Analysis class: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXECUTE - Encryption"
      ],
      "metadata": {
        "id": "nxucV5DZwDpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encrypting the general data"
      ],
      "metadata": {
        "id": "_62z7vAvs6PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "total_msg = \"No message to be displayed\"\n",
        "\n",
        "if query_class == 0:\n",
        "  total_count_stroke = df_stroke[\"stroke\"].value_counts()[1]\n",
        "  total_percentage_stroke = df_stroke[\"stroke\"].value_counts()[1] / (df_stroke[\"stroke\"].value_counts()[1] + df_stroke[\"stroke\"].value_counts()[0])\n",
        "  count_msg = \"Number of people in the total population with stroke \" + str(total_count_stroke)\n",
        "  percentage_msg = \"Percentage of people in the total population with stroke \" + str(total_percentage_stroke)\n",
        "  total_msg = count_msg + \"\\n\" + percentage_msg\n",
        "\n",
        "if query_class == 1:\n",
        "  given_id = 1665\n",
        "  ind = -1\n",
        "  for i in df_stroke.index:\n",
        "    if df_stroke[\"id\"][i] == given_id:\n",
        "      ind = i\n",
        "      break\n",
        "  if ind == -1:\n",
        "    print(\"No patient found with the given id\")\n",
        "  else:\n",
        "    total_msg = \"Gender: \" + str(df_stroke[\"gender\"][ind]) + \", Age: \" + str(df_stroke[\"age\"][ind])\n",
        "\n",
        "\n",
        "total_msg = total_msg.encode()\n",
        "\n",
        "key_role = rsa.generate_private_key(\n",
        "    public_exponent=65537,\n",
        "    key_size=2048,\n",
        ")\n",
        "\n",
        "\n",
        "ciphertext_role = key_role.public_key().encrypt(\n",
        "    total_msg,\n",
        "    padding.OAEP(\n",
        "        mgf=padding.MGF1(algorithm=hashes.SHA256()),\n",
        "        algorithm=hashes.SHA256(),\n",
        "        label=None\n",
        "    )\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Time taken to encrypt general role fence data \" + str(end_time - start_time))"
      ],
      "metadata": {
        "id": "wl8hLMTzs5jx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff3648a-cdfe-405e-d709-9db9e2093316"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to encrypt general role fence data 0.044239044189453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NL2SQL"
      ],
      "metadata": {
        "id": "cBrKHBt5wNOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the model\n",
        "\n",
        "def run_nl2sql(query):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"tscholak/3vnuv1vf\") \n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(\"tscholak/3vnuv1vf\")\n",
        "  pipe = pipeline('translation_xx_to_yy', model=model, tokenizer=tokenizer)\n",
        "  input = query + \" | strokeDB | stroke_table : id, gender, age, hypertension, heart_disease, ever_married, work_type, Residence_type, avg_glucose_level, bmi, smoking_status, stroke\"\n",
        "  start_time = time.time()\n",
        "  output = pipe(input)[0]['translation_text']\n",
        "  end_time = time.time()\n",
        "  return output, end_time - start_time"
      ],
      "metadata": {
        "id": "23SYLB9xwM1n"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis functions"
      ],
      "metadata": {
        "id": "DBLUiqD-Heg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gender_analysis(df):\n",
        "  count_male = 0\n",
        "  count_female = 0\n",
        "  count_other = 0\n",
        "  men = df[\"gender\"].value_counts()[\"Male\"]\n",
        "  women = df[\"gender\"].value_counts()[\"Female\"]\n",
        "  other = df[\"gender\"].value_counts()[\"Male\"]\n",
        "  for i in df.index:\n",
        "    if df[\"gender\"][i] == \"Male\" and df[\"stroke\"][i] == 1:\n",
        "      count_male += 1\n",
        "    if df[\"gender\"][i] == \"Female\" and df[\"stroke\"][i] == 1:\n",
        "      count_female += 1\n",
        "    if df[\"gender\"][i] == \"Other\" and df[\"stroke\"][i] == 1:\n",
        "      count_other += 1\n",
        "  \n",
        "  men_c = \"Number of men with stroke \" + str(count_male)\n",
        "  women_c = \"Number of wormen with stroke \" + str(count_female)\n",
        "  other_c = \"Number of people belonging to other genders with stroke \" + str(count_other)\n",
        "\n",
        "  men_p = \"Percentage of men with stroke \" + str(count_male*100 / men)\n",
        "  women_p = \"Percentage of wormen with stroke \" + str(count_female*100 / women)\n",
        "  other_p = \"Percentage of people belonging to other genders with stroke \" + str(count_other*100 / other)\n",
        "  return (men_c + \"\\n\" + women_c + \"\\n\" + other_c, men_p + \"\\n\" + women_p + \"\\n\" + other_p)\n",
        "\n",
        "def get_age_analysis(df):\n",
        "  count_age = [0, 0, 0, 0, 0 ,0 ,0 ,0 ,0 ,0]\n",
        "  age = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "  \n",
        "  for i in df.index:\n",
        "    val = df[\"age\"][i]\n",
        "    if val < 10:\n",
        "      age[0] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_age[0] += 1\n",
        "    elif val >= 10 and val < 20:\n",
        "      age[1] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_age[1] += 1\n",
        "    elif val >= 20 and val < 30:\n",
        "      age[2] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_age[2] += 1\n",
        "    elif val >= 30 and val < 40:\n",
        "      age[3] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_age[3] += 1\n",
        "    elif val >= 40 and val < 50:\n",
        "      age[4] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_age[4] += 1\n",
        "    elif val >= 50 and val < 60:\n",
        "      age[5] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_age[5] += 1\n",
        "    elif val >= 60 and val < 70:\n",
        "      age[6] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_age[6] += 1\n",
        "    elif val >= 70 and val < 80:\n",
        "      age[7] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_age[7] += 1\n",
        "    elif val >= 80 and val < 90:\n",
        "      age[8] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_age[8] += 1\n",
        "    elif val >= 90:\n",
        "      age[9] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_age[9] += 1\n",
        "\n",
        "  count_ana = \"Age Counts (stroke)\"\n",
        "  perc_ana = \"Age percentage (stroke)\"\n",
        "  t = 0\n",
        "  for i in range(10):\n",
        "    count_ana += \"Range \" + str(t) + \"-\" + str(t + 10) + \": \" + str(count_age[i]) + \"%\"\n",
        "    perc_ana += \"Range \" + str(t) + \"-\" + str(t + 10) + \": \" + str(count_age[i]*100 / age[i]) + \"%\"\n",
        "  return (count_ana, perc_ana)\n",
        "\n",
        "def get_hypertension_analysis(df):\n",
        "  count_hyp = 0\n",
        "  count_not_hyp = 0\n",
        "\n",
        "  hyp = df[\"hypertension\"].value_counts()[1]\n",
        "  not_hyp = df[\"hypertension\"].value_counts()[0]\n",
        "  for i in df.index:\n",
        "    if df[\"hypertension\"][i] == 1 and df[\"stroke\"][i] == 1:\n",
        "      count_hyp += 1\n",
        "    if df[\"hypertension\"][i] == 0 and df[\"stroke\"][i] == 1:\n",
        "      count_not_hyp += 1\n",
        "  hyp_c = \"Number of people with hypertension with stroke \" + str(count_hyp)\n",
        "  not_hyp_c = \"Number of people without hypertension with stroke \" + str(count_not_hyp)\n",
        "\n",
        "  hyp_p = \"Percentage of people with hypertension with stroke \" + str(count_hyp*100 / hyp)\n",
        "  not_hyp_p = \"Percentage of people without hypertension without stroke\" + str(count_not_hyp*100 / not_hyp)\n",
        "  return tuple((hyp_c + \"\\n\" + not_hyp_c, hyp_p + \"\\n\" + not_hyp_p))\n",
        "\n",
        "def get_heart_disease_analysis(df):\n",
        "  count_heart = 0\n",
        "  count_not_heart = 0\n",
        "\n",
        "  hyp = df[\"heart_disease\"].value_counts()[1]\n",
        "  not_hyp = df[\"heart_disease\"].value_counts()[0]\n",
        "  for i in df.index:\n",
        "    if df[\"heart_disease\"][i] == 1 and df[\"stroke\"][i] == 1:\n",
        "      count_heart += 1\n",
        "    if df[\"heart_disease\"][i] == 0 and df[\"stroke\"][i] == 1:\n",
        "      count_not_heart += 1\n",
        "  heart_c = \"Number of people with heart_disease with stroke \" + str(count_heart)\n",
        "  not_heart_c = \"Number of people without heart_disease with stroke \" + str(count_not_heart)\n",
        "\n",
        "  heart_p = \"Percentage of people with heart_disease with stroke \" + str(count_heart*100 / hyp)\n",
        "  not_heart_p = \"Percentage of people without heart_disease without stroke\" + str(count_not_heart*100 / not_hyp)\n",
        "  return (heart_c + \"\\n\" + not_heart_c, heart_p + \"\\n\" + not_heart_p)\n",
        "\n",
        "def get_married_analysis(df):\n",
        "  count_marr = 0\n",
        "  count_not_marr = 0\n",
        "\n",
        "  heart = df[\"ever_married\"].value_counts()[\"Yes\"]\n",
        "  not_heart = df[\"ever_married\"].value_counts()[\"No\"]\n",
        "  for i in df.index:\n",
        "    if df[\"ever_married\"][i] == \"Yes\" and df[\"stroke\"][i] == 1:\n",
        "      count_marr += 1\n",
        "    if df[\"ever_married\"][i] == \"No\" and df[\"stroke\"][i] == 1:\n",
        "      count_not_marr += 1\n",
        "  marr_c = \"Number of people who have ever married with stroke \" + str(count_marr)\n",
        "  not_marr_c = \"Number of people who have never married with stroke \" + str(count_not_marr)\n",
        "\n",
        "  marr_p = \"Percentage of people who have ever married with stroke \" + str(count_marr*100 / heart)\n",
        "  not_marr_p = \"Percentage of people who have never married without stroke\" + str(count_not_marr*100 / not_heart)\n",
        "  return (marr_c + \"\\n\" + not_marr_c, marr_p + \"\\n\" + not_marr_p)\n",
        "\n",
        "def get_living_analysis(df):\n",
        "  count_urban = 0\n",
        "  count_not_urban = 0\n",
        "\n",
        "  urb = df[\"Residence_type\"].value_counts()[\"Urban\"]\n",
        "  not_urb = df[\"Residence_type\"].value_counts()[\"Rural\"]\n",
        "  for i in df.index:\n",
        "    if df[\"Residence_type\"][i] == \"Urban\" and df[\"stroke\"][i]== 1:\n",
        "      count_urban += 1\n",
        "    if df[\"Residence_type\"][i] == \"Rural\" and df[\"stroke\"][i]== 1:\n",
        "      count_not_urban += 1\n",
        "  urban_c = \"Number of people with Residence_type urban with stroke \" + str(count_urban)\n",
        "  not_urban_c = \"Number of people with Residence_type rural with stroke \" + str(count_not_urban)\n",
        "\n",
        "  urban_p = \"Percentage of people with Residence_type urbam with stroke \" + str(count_urban*100 / urb)\n",
        "  not_urban_p = \"Percentage of people with Residence_type rural without stroke\" + str(count_not_urban*100 / not_urb)\n",
        "  return (urban_c + \"\\n\" + not_urban_c, urban_p + \"\\n\" + not_urban_p)\n",
        "\n",
        "def get_smoke_analysis(df):\n",
        "  count_smoke = 0\n",
        "  count_not_smoke = 0\n",
        "\n",
        "  smoke = df[\"smoking_status\"].value_counts()[\"formerly smoked\"]\n",
        "  not_smoke = df[\"smoking_status\"].value_counts()[\"never smoked\"]\n",
        "  for i in df.index:\n",
        "    if df[\"smoking_status\"][i] == \"formerly smoked\" and df[\"stroke\"][i] == 1:\n",
        "      count_smoke += 1\n",
        "    if df[\"smoking_status\"][i] == \"never smoked\" and df[\"stroke\"][i] == 1:\n",
        "      count_not_smoke += 1\n",
        "  smoke_c = \"Number of people who have smoked with stroke \" + str(count_smoke)\n",
        "  not_smoke_c = \"Number of people who have never smoked with stroke \" + str(count_not_smoke)\n",
        "\n",
        "  smoke_p = \"Percentage of people who have smoked with stroke \" + str(count_smoke*100 / smoke)\n",
        "  not_smoke_p = \"Percentage of people who have never smoked without stroke\" + str(count_not_smoke*100 / not_smoke)\n",
        "  return (smoke_c + \"\\n\" + not_smoke_c, smoke_p + \"\\n\" + not_smoke_p)\n",
        "\n",
        "def get_bmi_analysis(df):\n",
        "  count_bmi = [0, 0, 0]\n",
        "  bmi = [0, 0, 0]\n",
        "  \n",
        "  for i in df.index:\n",
        "    val = df[\"bmi\"][i]\n",
        "    if val < 20:\n",
        "      bmi[0] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_bmi[0] += 1\n",
        "    elif val >= 20 and val < 25:\n",
        "      bmi[1] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_bmi[1] += 1\n",
        "    elif val >= 25:\n",
        "      bmi[2] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_bmi[2] += 1\n",
        "\n",
        "  count_ana = \"bmi Counts (stroke)\\n\"\n",
        "  perc_ana = \"bmi percentage (stroke)\\n\"\n",
        "  count_ana += \"Range 0 - 20 : \" + str(count_bmi[0]) + \"%\\n\"\n",
        "  perc_ana += \"Range 0 - 20 : \" + str(count_bmi[0]*100 / bmi[0]) + \"%\\n\"\n",
        "  count_ana += \"Range 20-25 : \" + str(count_bmi[1]) + \"%\\n\"\n",
        "  perc_ana += \"Range 20-25 : \" + str(count_bmi[1]*100 / bmi[1]) + \"%\\n\"\n",
        "  count_ana += \"Range > 25 : \" + str(count_bmi[2]) + \"%\"\n",
        "  perc_ana += \"Range > 25 : \" + str(count_bmi[2]*100 / bmi[2]) + \"%\"\n",
        "    \n",
        "  return (count_ana, perc_ana)\n",
        "\n",
        "def get_work_analysis(df):\n",
        "  count_work = [0, 0, 0]\n",
        "  work = [0, 0, 0]\n",
        "  \n",
        "  for i in df.index:\n",
        "    val = df[\"work_type\"][i]\n",
        "    if val == \"Private\":\n",
        "      work[0] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_work[0] += 1\n",
        "    elif val == \"Self-employed\":\n",
        "      work[1] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_work[1] += 1\n",
        "    elif val == \"Other\":\n",
        "      work[2] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_work[2] += 1\n",
        "\n",
        "  count_ana = \"work Counts (stroke)\\n\"\n",
        "  perc_ana = \"work percentage (stroke)\\n\"\n",
        "  count_ana += \"Private: \" + str(count_work[0]) + \"%\\n\"\n",
        "  perc_ana += \"Private: \" + str(count_work[0]*100 / work[0]) + \"%\\n\"\n",
        "  count_ana += \"Self-employed : \" + str(count_work[1]) + \"%\\n\"\n",
        "  perc_ana += \"Self-employed : \" + str(count_work[1]*100 / work[1]) + \"%\\n\"\n",
        "  count_ana += \"Other : \" + str(count_work[2]) + \"%\"\n",
        "  perc_ana += \"Other : \" + str(count_work[2]*100 / work[2]) + \"%\"\n",
        "    \n",
        "  return (count_ana, perc_ana)\n",
        "\n",
        "def get_glucose_analysis(df):\n",
        "  count_glucose = [0, 0, 0]\n",
        "  Glucose = [0, 0, 0]\n",
        "  \n",
        "  for i in df.index:\n",
        "    val = df[\"avg_glucose_level\"][i]\n",
        "    if val < 50:\n",
        "      Glucose[0] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_glucose[0] += 1\n",
        "    elif val >= 50 and val < 100:\n",
        "      Glucose[1] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_glucose[1] += 1\n",
        "    elif val >= 100 and val < 150:\n",
        "      Glucose[2] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_glucose[2] += 1\n",
        "    elif val >= 150:\n",
        "      Glucose[3] += 1\n",
        "      if df[\"stroke\"][i] == 1:\n",
        "        count_glucose[3] += 1\n",
        "\n",
        "  count_ana = \"Glucose Counts (stroke)\\n\"\n",
        "  perc_ana = \"Glucose percentage (stroke)\\n\"\n",
        "  count_ana += \"Range 0 - 50 : \" + str(count_glucose[0]) + \"%\\n\"\n",
        "  perc_ana += \"Range 0 - 50 : \" + str(count_glucose[0]*100 / Glucose[0]) + \"%\\n\"\n",
        "  count_ana += \"Range 50-100 : \" + str(count_glucose[1]) + \"%\\n\"\n",
        "  perc_ana += \"Range 50-100 : \" + str(count_glucose[1]*100 / Glucose[1]) + \"%\\n\"\n",
        "  count_ana += \"Range 100-150 : \" + str(count_glucose[2]) + \"%\\n\"\n",
        "  perc_ana += \"Range 100-150 : \" + str(count_glucose[2]*100 / Glucose[2]) + \"%\\n\"\n",
        "  count_ana += \"Range >= 150 : \" + str(count_glucose[3]) + \"%\"\n",
        "  perc_ana += \"Range >= 150 : \" + str(count_glucose[3]*100 / Glucose[3]) + \"%\"\n",
        "    \n",
        "  return (count_ana, perc_ana)\n",
        "\n"
      ],
      "metadata": {
        "id": "n7lCancJHeRT"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_analysis(column, df):\n",
        "  if column == \"gender\":\n",
        "    out = get_gender_analysis(df)\n",
        "    return out\n",
        "  elif column == \"age\":\n",
        "    out = get_age_analysis(df)\n",
        "    return out\n",
        "  elif column == \"hypertension\":\n",
        "    out = get_hypertension_analysis(df)\n",
        "    return tuple(out)\n",
        "  elif column == \"heart_disease\":\n",
        "    out = get_heart_disease_analysis(df)\n",
        "    return out\n",
        "  elif column == \"ever_married\":\n",
        "    out = get_married_analysis(df)\n",
        "    return out\n",
        "  elif column == \"bmi\":\n",
        "    out = get_bmi_analysis(df)\n",
        "    return out\n",
        "  elif column == \"smoking_status\":\n",
        "    out = get_smoke_analysis(df)\n",
        "    return out\n",
        "  elif column == \"Residence_type\":\n",
        "    out = get_living_analysis(df)\n",
        "    return out\n",
        "  elif column == \"work_type\":\n",
        "    out = get_work_analysis(df)\n",
        "    return out\n",
        "  elif column == \"avg_glucose_level\":\n",
        "    out = get_glucose_analysis(df)\n",
        "    return out\n",
        "\n",
        "def get_correlation(column1, column2, df):\n",
        "  return df.corr()[column1][column2]"
      ],
      "metadata": {
        "id": "t3Jpj__XHdPp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encrypting final outputs"
      ],
      "metadata": {
        "id": "Or7YhO0Cx0Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_output = \"\"\n",
        "\n",
        "\n",
        "# Analytics\n",
        "if query_class == 0:\n",
        "  data = \"\"\n",
        "  if query_analysis_class == 1:\n",
        "    for i in df_stroke.columns:\n",
        "      column_name = i.replace(\"_\", \" \")\n",
        "      if column_name.lower() in input_query.lower()and i != \"stroke\":\n",
        "        temp = get_analysis(i, df_stroke)\n",
        "        data += (temp[1] + \"\\n\")\n",
        "  elif query_analysis_class == 0:\n",
        "    for i in df_stroke.columns:\n",
        "      column_name = i.replace(\"_\", \" \")\n",
        "      if column_name.lower() in input_query.lower() and i != \"stroke\":\n",
        "        temp = get_analysis(i, df_stroke)\n",
        "        data += (temp[0] + \"\\n\")\n",
        "  else:\n",
        "    column1 = \"\"\n",
        "    column2 = \"\"\n",
        "    for i in df_stroke.columns:\n",
        "      column_name = i.replace(\"_\", \" \")\n",
        "      if column_name in input_query.lower():\n",
        "        if column1 == \"\":\n",
        "          column1 = i\n",
        "        elif column1 != \"\" and column2 == \"\":\n",
        "          column2 = i\n",
        "      if column1 != \"\" and column2 != \"\":\n",
        "        break\n",
        "    data = \"Correlation between {0} and {1}: {2}\".format(column1, column2, get_correlation(column1, column2, df_stroke))\n",
        "  final_output = data\n",
        "\n",
        "if query_class == 1:\n",
        "  sql_query, t = run_nl2sql(query_without_purpose)\n",
        "  print(\"Time taken to run nl2sql: \" + str(t))\n",
        "  start_time = time.time()\n",
        "  crsr.execute(sql_query[11:])\n",
        "  end_time = time.time()\n",
        "  print(\"Time taken to fetch the query: \" + str(end_time - start_time))\n",
        "  final_output = str(crsr.fetchall())\n",
        "\n",
        "\n",
        "if query_class == 2:\n",
        "  df_stroke_pred = df_stroke.copy()\n",
        "  df_stroke_pred.drop('id', axis = 1, inplace =True)\n",
        "  input_array = []\n",
        "  # Input order\n",
        "  # gender, age, hypertension, heart_disease,\tever_married,\twork_type,\tResidence_type,\tavg_glucose_level,\tbmi,\tsmoking_status\n",
        "  input_array.append(input(\"Enter the gender: \"))\n",
        "  input_array.append(int(input(\"Enter the age: \")))\n",
        "  input_array.append(int(input(\"Does the patient have hypertension? (0/1): \")))\n",
        "  input_array.append(int(input(\"Does the patient have heart_disease? (0/1): \")))\n",
        "  input_array.append(input(\"Has the patient ever married? (Yes/No): \"))\n",
        "  input_array.append(input(\"Patient work type: \"))\n",
        "  input_array.append(input(\"Patient residence type (Urban / Rural): \"))\n",
        "  input_array.append(float(input(\"average glucose level: \")))\n",
        "  input_array.append(float(input(\"BMI: \")))\n",
        "  input_array.append(input(\"Smoking status: \"))\n",
        "\n",
        "\n",
        "  # Encode Categorical Data\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "  label_encoder.fit(df_stroke_pred['gender'])\n",
        "  input_array[0] = label_encoder.transform([input_array[0]])[0]\n",
        "  df_stroke_pred['gender'] = label_encoder.transform(df_stroke_pred[\"gender\"])\n",
        "\n",
        "  label_encoder.fit(df_stroke_pred['ever_married'])\n",
        "  input_array[4] = label_encoder.transform([input_array[4]])[0]\n",
        "  df_stroke_pred['ever_married'] = label_encoder.transform(df_stroke_pred[\"ever_married\"])\n",
        "\n",
        "  label_encoder.fit(df_stroke_pred['work_type'])\n",
        "  input_array[5] = label_encoder.transform([input_array[5]])[0]\n",
        "  df_stroke_pred['work_type'] = label_encoder.transform(df_stroke_pred[\"work_type\"])\n",
        "\n",
        "  label_encoder.fit(df_stroke_pred['Residence_type'])\n",
        "  input_array[6] = label_encoder.transform([input_array[6]])[0]\n",
        "  df_stroke_pred['Residence_type'] = label_encoder.transform(df_stroke_pred[\"Residence_type\"])\n",
        "\n",
        "  label_encoder.fit(df_stroke_pred['smoking_status'])\n",
        "  input_array[9] = label_encoder.transform([input_array[9]])[0]\n",
        "  df_stroke_pred['smoking_status'] = label_encoder.transform(df_stroke_pred[\"smoking_status\"])\n",
        "\n",
        "  df_stroke['bmi'].fillna(value = df_stroke['bmi'].mean(), inplace = True)\n",
        "\n",
        "  model = predictionmodel.train_model(df_stroke_pred)\n",
        "  prediction = predictionmodel.predict(model, [input_array])[0]\n",
        "  if prediction == 1:\n",
        "    final_output = \"The user will have a stroke\"\n",
        "  else:\n",
        "    final_output = \"The user will not have a stroke\"\n",
        "\n",
        "final_key = rsa.generate_private_key(\n",
        "    public_exponent=65537,\n",
        "    key_size=2048,\n",
        ")\n",
        "\n",
        "ciphertext_final = final_key.public_key().encrypt(\n",
        "    final_output.encode(),\n",
        "    padding.OAEP(\n",
        "        mgf=padding.MGF1(algorithm=hashes.SHA256()),\n",
        "        algorithm=hashes.SHA256(),\n",
        "        label=None\n",
        "    )\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr5ylY1JxyPK",
        "outputId": "2aae5531-c843-4abf-8948-9c0fc628c73d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to run nl2sql: 23.77999472618103\n",
            "Time taken to fetch the query: 5.2928924560546875e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enforce"
      ],
      "metadata": {
        "id": "8VDie5-0xguP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the purpose table\n",
        "purpose_table = None\n",
        "purpose_role_table = None\n",
        "if curr_user.institution == \"INSURANCE\":\n",
        "  purpose_table = purpose_table_insurance\n",
        "  purpose_role_table = purpose_role_insurance\n",
        "\n",
        "if curr_user.institution == \"HOSPITAL\":\n",
        "  purpose_table = purpose_table_hospital\n",
        "  purpose_role_table = purpose_role_hospital\n",
        "\n",
        "if curr_user.institution == \"RESEARCH\":\n",
        "  purpose_table = purpose_table_research\n",
        "  purpose_role_table = purpose_role_research"
      ],
      "metadata": {
        "id": "_CQJNhBN-Qdw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collecting keys to decrypt data\n",
        "keys_collected = []\n",
        "decryption_keys = []"
      ],
      "metadata": {
        "id": "lc6wXRod7kT0"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the role and institution"
      ],
      "metadata": {
        "id": "T9gG1O4RnnOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "inst_fence = False\n",
        "role_fence = False\n",
        "\n",
        "if query_class_text in institution_query_types[curr_user.institution]:\n",
        "  inst_fence = True\n",
        "\n",
        "if query_class_text in purpose_role_table[curr_user.role]:\n",
        "  role_fence = True\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Time taken for role and institution fence: \", str(end_time - start_time))\n",
        "\n",
        "if not inst_fence:\n",
        "  print(\"Access denied at institute fence\")\n",
        "\n",
        "if not inst_fence and role_fence:\n",
        "  print(\"Access denied at role fence\")"
      ],
      "metadata": {
        "id": "ryA2PR3pyHzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b60e3f-b3ff-4f60-ee59-5208f8a3482a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for role and institution fence:  0.00023508071899414062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if role_fence and inst_fence:\n",
        "  keys_collected.append(\"role_key\")\n",
        "\n",
        "# key = Fernet.generate_key()\n",
        "# fernet = Fernet(key)\n",
        "# enc_total_count = fernet.encrpyt(total_count_stroke.encode())\n",
        "# decryption_keys[\"total_count\"] = [enc_total_count, key]\n",
        "\n",
        "# key = Fernet.generate_key()\n",
        "# fernet = Fernet(key)\n",
        "# enc_total_percentage = fernet.encrypt(total_percentage_stroke.encode())\n",
        "# decryption_keys[\"total_percentage\"] = [enc_total_percentage, key]\n",
        "# decryption_keys[\"role_layer\": [ciphertext_role, key_role]]"
      ],
      "metadata": {
        "id": "NiUb8Eds7c7U"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the purpose"
      ],
      "metadata": {
        "id": "v5RbshID8EcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contracts = purpose_table[extracted_purpose]"
      ],
      "metadata": {
        "id": "jxfrfqckAZQa"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "purpose_fence = False\n",
        "if curr_user.role in contracts[0] and query_class_text in contracts[0][curr_user.role]:\n",
        "  purpose_fence = True\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Time taken at purpose fence: \" + str(end_time - start_time))\n",
        "\n",
        "if not purpose_fence:\n",
        "  print(\"Access denied at purpose fence\")"
      ],
      "metadata": {
        "id": "gyjv8qMu8EyW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7dec09-e6ab-4e07-9ae5-5130b6be3bd2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken at purpose fence: 0.00022220611572265625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Location and time conditions"
      ],
      "metadata": {
        "id": "SV_0zyOMA5np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_fence = False\n",
        "location_fence = False\n",
        "\n",
        "start_time = time.time()\n",
        "if curr_user.location in contracts[2]:\n",
        "  location_fence = True\n",
        "\n",
        "curr_time = int(time.strftime(\"%H\"))\n",
        "\n",
        "if curr_time > contracts[1][0] and curr_time < contracts[1][1]:\n",
        "  time_fence = True\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Time taken at time and location fences: \" + str(end_time - start_time))\n",
        "\n",
        "if not time_fence:\n",
        "  print(\"Failed at time fence\")\n",
        "if not location_fence:\n",
        "  print(\"Failed at location fence\")"
      ],
      "metadata": {
        "id": "eHJGQTV9AHIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c733fdb-e84b-470c-a9de-f050dc73632f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken at time and location fences: 0.0002899169921875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if purpose_fence and time_fence and location_fence and role_fence and inst_fence:\n",
        "  keys_collected.append(\"final_key\")"
      ],
      "metadata": {
        "id": "DtSSuQe7cl4B"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXECUTE - Decryption"
      ],
      "metadata": {
        "id": "b36Ez_xBFo5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decrypting the data"
      ],
      "metadata": {
        "id": "t0Wtyd4pd_2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decrypt_data(ciphertext, key):\n",
        "\n",
        "  plaintext = key.decrypt(\n",
        "      ciphertext,\n",
        "      padding.OAEP(\n",
        "          mgf=padding.MGF1(algorithm=hashes.SHA256()),\n",
        "          algorithm=hashes.SHA256(),\n",
        "          label=None\n",
        "      )\n",
        "  )\n",
        "  return plaintext.decode()"
      ],
      "metadata": {
        "id": "uiylZ8FrGvLt"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def release_general_analysis(decryption_keys):\n",
        "  key = decryption_keys[\"role_layer\"][1]\n",
        "  ciphertext = decryption_keys[\"role_layer\"][0]\n",
        "  plaintext = key.decrypt(\n",
        "      ciphertext,\n",
        "      padding.OAEP(\n",
        "          mgf=padding.MGF1(algorithm=hashes.SHA256()),\n",
        "          algorithm=hashes.SHA256(),\n",
        "          label=None\n",
        "      )\n",
        "  )\n",
        "  print(plaintext.decode())"
      ],
      "metadata": {
        "id": "32A8-cIHeDAO"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Releasing the data"
      ],
      "metadata": {
        "id": "iA8foMEFdp8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Releasing the data\n",
        "\n",
        "start_time = time.time()\n",
        "if query_class == 0 or query_class == 1:\n",
        "  # ANALYSIS\n",
        "  if \"role_key\" in keys_collected:\n",
        "    print(decrypt_data(ciphertext_role, key_role))\n",
        "\n",
        "if \"final_key\" in keys_collected:\n",
        "  print(decrypt_data(ciphertext_final, final_key))\n",
        "end_time = time.time()\n",
        "print(\"Time taken to decrypt and release the data: \" + str(end_time - start_time))\n"
      ],
      "metadata": {
        "id": "lvLpqASxFpL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d735189e-a642-4abb-b31d-27e0fda1ab4a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gender: Female, Age: 79.0\n",
            "[(24,)]\n",
            "Time taken to decrypt and release the data: 0.014449119567871094\n"
          ]
        }
      ]
    }
  ]
}
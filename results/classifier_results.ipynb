{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8YwtpHuECDU",
        "outputId": "e25cefc0-a4f2-4494-ea3a-cdac5db8ca58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=6c3af28d6a11d507b9a5cf2f1ba331ee25e8477f88f6dabb927fa8bb785fb114\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.13.3 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yb6qxhVAEQPp"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
        "from google.colab import files\n",
        "from google.colab import data_table\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from transformers import BertTokenizer, BertForTokenClassification, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0PZf5P8fEtuV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Ring_fencing_files/Final_query_dataset - Sheet1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4fJt0kWNEmqW"
      },
      "outputs": [],
      "source": [
        "def get_bert_embedding(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the bert embedding vectors for the each sentence column.\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "  cont_model = SentenceTransformer('distilbert-base-uncased')\n",
        "  \n",
        "  feature1 = cont_model.encode(data_frame)\n",
        "  \n",
        "  return feature1\n",
        "\n",
        "column = \"LABEL\"\n",
        "df_enc = df.copy()\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df[column].unique())\n",
        "df_enc[column] = le.transform(df[column])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_qEHDLnFCYc"
      },
      "source": [
        "k-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DlxqoryeFCDn"
      },
      "outputs": [],
      "source": [
        "def run_4_folds(clf, df):\n",
        "  size = len(df.index) // 4\n",
        "  start = 0\n",
        "  folds = []\n",
        "  df = df.sample(frac = 1)\n",
        "  for i in range(3):\n",
        "    folds.append(df.iloc[start:start + size, :])\n",
        "    start += size\n",
        "  folds.append(df.iloc[start:, :])\n",
        "  f1_scores = []\n",
        "  accuracies = []\n",
        "  prec = []\n",
        "  rec = []\n",
        "  for i in range(4):\n",
        "    temp = folds.copy()\n",
        "    df_test = temp.pop(i)\n",
        "    df_train = pd.concat(temp)\n",
        "    X_train = df_train[\"QUERY\"]\n",
        "    y_train = df_train[\"LABEL\"]\n",
        "    X_test = df_test[\"QUERY\"]\n",
        "    y_test = df_test[\"LABEL\"]\n",
        "    feature_1_train = get_bert_embedding(np.array(X_train))\n",
        "    if clf == \"svc\":\n",
        "      model_classify = SVC(kernel = \"sigmoid\")\n",
        "    if clf == \"lr\":\n",
        "      model_classify = LogisticRegression(max_iter = 500)\n",
        "    if clf == \"mlp\":\n",
        "      model_classify = MLPClassifier(hidden_layer_sizes = (256, 128, 64), activation = \"logistic\")\n",
        "    if clf == \"dt\":\n",
        "      model_classify = DecisionTreeClassifier(criterion = \"entropy\")\n",
        "    model_classify.fit(np.array(feature_1_train), y_train)\n",
        "    feature_1_test = get_bert_embedding(np.array(X_test))\n",
        "    preds = model_classify.predict(feature_1_test)\n",
        "    f1_scores.append(f1_score(y_test, preds, average = \"macro\"))\n",
        "    accuracies.append(accuracy_score(y_test, preds))\n",
        "    prec.append(precision_score(y_test, preds, average = \"macro\"))\n",
        "    rec.append(recall_score(y_test, preds, average = \"macro\"))\n",
        "  return sum(f1_scores) / 4, sum(accuracies) / 4, sum(prec) / 4, sum(rec) / 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-8T4ffdwB04"
      },
      "source": [
        "SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_YL-rydIGR6"
      },
      "outputs": [],
      "source": [
        "f1, acc, prec, rec = run_4_folds(\"svc\", df_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS1T8zT_LHvn",
        "outputId": "abd6a0b7-0ff7-486f-c35b-215089218ecd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.9498863836768433\n",
            "Accuracy: 0.951092712893659\n",
            "Precision: 0.9501502472067493\n",
            "Recall: 0.9522696364226019\n"
          ]
        }
      ],
      "source": [
        "print(\"F1 score: \" + str(f1))\n",
        "print(\"Accuracy: \" + str(acc))\n",
        "print(\"Precision: \"+ str(prec))\n",
        "print(\"Recall: \"+ str(rec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr0lom-xwEoQ"
      },
      "source": [
        "Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l3nuWo4MO1f"
      },
      "outputs": [],
      "source": [
        "f1, acc, prec, rec = run_4_folds(\"lr\", df_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUDhtCWRMRrX",
        "outputId": "61432699-5554-412c-bd5f-6b51fb7e44e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.9923904081985944\n",
            "Accuracy: 0.9922768676740573\n",
            "Precision: 0.992524499959678\n",
            "Recall: 0.9923732300476037\n"
          ]
        }
      ],
      "source": [
        "print(\"F1 score: \" + str(f1))\n",
        "print(\"Accuracy: \" + str(acc))\n",
        "print(\"Precision: \"+ str(prec))\n",
        "print(\"Recall: \"+ str(rec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYC-9PbLBX1H"
      },
      "source": [
        "MLP classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfDomeElMziv"
      },
      "outputs": [],
      "source": [
        "f1, acc, prec, rec = run_4_folds(\"mlp\", df_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_FPXIV6NCwg",
        "outputId": "c17875ef-5471-403b-bf5f-238b3bcec971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.9908285391131013\n",
            "Accuracy: 0.9905586546156381\n",
            "Precision: 0.9908408920444115\n",
            "Recall: 0.9908869839016801\n"
          ]
        }
      ],
      "source": [
        "print(\"F1 score: \" + str(f1))\n",
        "print(\"Accuracy: \" + str(acc))\n",
        "print(\"Precision: \"+ str(prec))\n",
        "print(\"Recall: \"+ str(rec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEQdsg-nwJpk"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTCcV8ncwY-7"
      },
      "outputs": [],
      "source": [
        "f1, acc, prec, rec = run_4_folds(\"dt\", df_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FT2_g1NwZV3",
        "outputId": "88195a7f-3e9d-4450-f5b8-a908c581be1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.8966921119668976\n",
            "Accuracy: 0.898701925340112\n",
            "Precision: 0.8965477626645985\n",
            "Recall: 0.8976296113895434\n"
          ]
        }
      ],
      "source": [
        "print(\"F1 score: \" + str(f1))\n",
        "print(\"Accuracy: \" + str(acc))\n",
        "print(\"Precision: \"+ str(prec))\n",
        "print(\"Recall: \"+ str(rec))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
